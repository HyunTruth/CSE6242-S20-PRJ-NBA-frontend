{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dictionaries to be used to map ID values in dataframe to meaningful play descriptions. NOTE: these are incomplete and need to be fleshed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_types = {\n",
    "    1: 'made shot',\n",
    "    2: 'missed shot',\n",
    "    3: 'free throw',\n",
    "    4: 'rebound',\n",
    "    5: 'turnover',\n",
    "    6: 'personal foul',\n",
    "    8: 'substitution',\n",
    "    9: 'timeout',\n",
    "    10: 'jump ball',\n",
    "    12: 'start period',\n",
    "    13: 'end period',\n",
    "    18: 'instant replay',\n",
    "    20: 'stoppage'\n",
    "}\n",
    "\n",
    "opt1_types = {\n",
    "    3: '3_pt'\n",
    "}\n",
    "\n",
    "etypes = {\n",
    "    1: 'made',\n",
    "    2: 'missed'\n",
    "}\n",
    "\n",
    "mtypes = {\n",
    "    5: 'layup',\n",
    "    6: 'driving layup',\n",
    "    # 7 appears to include dunks and discontinued dribbles?\n",
    "    9: 'driving dunk',\n",
    "    # 11 is a ton of stuff, but mostly 1st turnovers?\n",
    "    # 12 is mostly 2nd turnovers\n",
    "    13: 'third free throw',\n",
    "    58: 'turnaround hook'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retrieve the list of players and teams for the season from the NBA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_endpoint_url = \"http://data.nba.net/10s/prod/v1/2016/players.json\"\n",
    "players_json = json.loads(requests.get(player_endpoint_url).content)\n",
    "teams_endpoint_url = \"http://data.nba.net/10s/prod/v1/2016/teams.json\"\n",
    "teams_json = json.loads(requests.get(teams_endpoint_url).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "turn the player data into tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select just the players that played during the regular season, not summer league or preseason nonsense\n",
    "raw_players = json_normalize(players_json['league']['standard'])\n",
    "# get a subset of the player data to joing to pbp tables\n",
    "if 'temporaryDisplayName' in list(raw_players.columns):\n",
    "    player_names = raw_players[['personId', 'temporaryDisplayName']]\n",
    "else:\n",
    "    player_names = raw_players[['personId', 'firstName', 'lastName']]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "turn team data into tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get just the teams from the regular season\n",
    "raw_teams = json_normalize(teams_json['league']['standard'])\n",
    "# subset the team names to join back to play-by-play data\n",
    "team_names = raw_teams[['fullName', 'teamId']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create function that takes a game ID and returns a dataframe of play-by-play data. Note that this intentionally returns a very raw copy of the dataframe, the idea being we can save the raw data to flat files/pickle files and reprocess them with additional fields and mappings as needed as we figure out more ways into the day. Hit the API once, process later as many times as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_pbp_df(game_id, year = '2019'):\n",
    "    # construct the url\n",
    "    game_url = \"https://data.nba.com/data/10s/v2015/json/mobile_teams/nba/\" + year + \"/scores/pbp/\" + game_id + \"_full_pbp.json\"\n",
    "    # make the API call and retreive the contents as a string\n",
    "    try:\n",
    "        pbp_raw_content = requests.get(game_url).content\n",
    "        # extract the bytestring into a json object (a python dict)\n",
    "        pbp_json = json.loads(pbp_raw_content)\n",
    "        # crete a list of dataframes from the raw pbp data\n",
    "        #   the g.pd is a list of game periods\n",
    "        #   we call json_normalize to turn each list of play actions in a period into a dataframe\n",
    "        pbp_periods = [json_normalize(quarter['pla']) for quarter in pbp_json['g']['pd']]\n",
    "        # concat the periods together as one dataframe\n",
    "        play_by_play_raw = (pd.concat(pbp_periods)\n",
    "                            # make sure the player ID gets stored as a string- there's no need\n",
    "                            #   to save it as a number and it messes with later joins where the\n",
    "                            #   type is object because not all elements are populated\n",
    "                              .assign(pid = lambda df: df.pid.astype(str),\n",
    "                            # append the game id onto the dataframe\n",
    "                                      game_id = game_id)\n",
    "                           )\n",
    "        # sleep for a few seconds to rate limit how often we hit the API\n",
    "        time.sleep(2)\n",
    "        return play_by_play_raw\n",
    "    except Exception as e:\n",
    "        print(game_url)\n",
    "        print(e)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create lists of game IDs to retrieve.\n",
    "\n",
    "Format: 00(last 2 digits of starting season)(number of game, ordered by start time of season)\n",
    "ex. 00190001 is the first game of the 2019-2020 season, 00190500 is the 500th game of the 2019-2020 season."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retrieve most recent ~50 games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "game_ids1 = [str(n).zfill(10) for n in range(21500001,21500100)]\n",
    "game_dfs1 = [retrieve_pbp_df(game_id, '2015') for game_id in game_ids1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ids2 = [str(n).zfill(10) for n in range(21600101,21600200)]\n",
    "game_dfs2 = [retrieve_pbp_df(game_id, '2016') for game_id in game_ids2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ids3 = [str(n).zfill(10) for n in range(21600201,21600300)]\n",
    "game_dfs3 = [retrieve_pbp_df(game_id, '2016') for game_id in game_ids3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ids4 = [str(n).zfill(10) for n in range(21600301,21600400)]\n",
    "game_dfs4 = [retrieve_pbp_df(game_id, '2016') for game_id in game_ids4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ids5 = [str(n).zfill(10) for n in range(21600401,21600500)]\n",
    "game_dfs5 = [retrieve_pbp_df(game_id, '2016') for game_id in game_ids5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ids6 = [str(n).zfill(10) for n in range(21600501,21600600)]\n",
    "game_dfs6 = [retrieve_pbp_df(game_id, '2016') for game_id in game_ids6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ids7 = [str(n).zfill(10) for n in range(21600601,21600700)]\n",
    "game_dfs7 = [retrieve_pbp_df(game_id, '2016') for game_id in game_ids7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ids8 = [str(n).zfill(10) for n in range(21600701,21600800)]\n",
    "game_dfs8 = [retrieve_pbp_df(game_id, '2016') for game_id in game_ids8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ids9 = [str(n).zfill(10) for n in range(21600801,21600900)]\n",
    "game_dfs9 = [retrieve_pbp_df(game_id, '2016') for game_id in game_ids9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ids10 = [str(n).zfill(10) for n in range(21600901,21601000)]\n",
    "game_dfs10 = [retrieve_pbp_df(game_id, '2016') for game_id in game_ids9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ids11 = [str(n).zfill(10) for n in range(21601001,21601230)]\n",
    "game_dfs11 = [retrieve_pbp_df(game_id, '2016') for game_id in game_ids11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_games = [game_dfs1, game_dfs2, game_dfs3, game_dfs4, game_dfs5, game_dfs6, game_dfs7, game_dfs8, game_dfs9, game_dfs10, game_dfs11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_game_rows = pd.concat([pd.concat(x) for x in all_games])\n",
    "all_game_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_game_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_game_rows.to_csv('2016_pbp.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modify the below cell if you would like to apply the field mapping to a differnt subset of games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_by_play_raw = all_game_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge the player names onto the dataset, joining on the player ID fields. Same logic could be done for team ID fields, just didn't get that far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "play_by_play_before_maps = (play_by_play_raw\n",
    " .merge(player_names\n",
    "            .rename({\n",
    "                'personId':'opid',\n",
    "                'temporaryDisplayName': 'opName'},\n",
    "                axis='columns'),\n",
    "        how = 'left', \n",
    "        on='opid')\n",
    " .merge(player_names\n",
    "            .rename({\n",
    "                'personId': 'pid',\n",
    "                'temporaryDisplayName': 'pName'},\n",
    "                axis='columns'),\n",
    "       how = 'left',\n",
    "       on = 'pid')\n",
    " .merge(player_names\n",
    "           .rename({\n",
    "               'personId': 'epid',\n",
    "               'temporaryDisplayName': 'epName'},\n",
    "               axis='columns'),\n",
    "       how='left',\n",
    "       on='epid')\n",
    "\n",
    "play_by_play_before_maps.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add additional fields based on the dictionary mappings above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_by_play_with_maps = (play_by_play_before_maps\n",
    " .assign(event_type = lambda df: df.etype.map(event_types),\n",
    "         opt1_type = lambda df: df.opt1.map(opt1_types),\n",
    "         etype_type = lambda df: df.etype.map(etypes),\n",
    "         mtypes = lambda df: df.mtype.map(mtypes)))\n",
    "play_by_play_with_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old single-game code used to test logic before slamming the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_pbp_url = \"https://data.nba.com/data/10s/v2015/json/mobile_teams/nba/2019/scores/pbp/0021900958_full_pbp.json\"\n",
    "pbp_json = json.loads(requests.get(sample_pbp_url).content)\n",
    "#json_normalize(pbp_json['g']['pd'])\n",
    "x = [json_normalize(quarter['pla']) for quarter in pbp_json['g']['pd']]\n",
    "play_by_play_raw = pd.concat(x).assign(pid = lambda df: df.pid.astype(str))\n",
    "play_by_play_raw.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
